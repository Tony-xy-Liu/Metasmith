{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KeyGenerator:\n",
    "    def __init__(self, full=False, seed=None) -> None:\n",
    "        ascii_vocab = [(48, 57), (65, 90), (97, 122)]\n",
    "        vocab = [chr(i) for g in [range(a, b+1) for a, b in ascii_vocab] for i in g]\n",
    "        if full: vocab += [c for c in \"-_\"]\n",
    "        self.vocab = vocab\n",
    "        self._generator = np.random.default_rng(seed)\n",
    "\n",
    "    def GenerateUID(self, l:int=8, blacklist: set[str]=set()) -> str:\n",
    "        key: str|None = None\n",
    "        while key is None or key in blacklist:\n",
    "            digits = self._generator.integers(0, len(self.vocab), l)\n",
    "            key = \"\".join([self.vocab[i] for i in digits])\n",
    "        return key\n",
    "    \n",
    "    def FromInt(self, i: int, l: int=8, little_endian=False):\n",
    "        chunks = [self.vocab[0]]*l\n",
    "        place = 0\n",
    "        while i > 0 and place < l:\n",
    "            chunk_k = i % len(self.vocab)\n",
    "            i = (i - chunk_k) // len(self.vocab)\n",
    "            chunks[place] = self.vocab[chunk_k]\n",
    "            place += 1\n",
    "        if not little_endian: chunks.reverse()\n",
    "        return \"\".join(chunks)\n",
    "    \n",
    "    def FromHex(self, hex: str, l: int=8, little_endian=False):\n",
    "        return self.FromInt(int(hex, 16), l, little_endian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 solutions\n",
      "step 1\n",
      "{a}->{b}\n",
      "used\n",
      "  (a:gmlC)\n",
      "produced\n",
      "  (b:wR9p)\n",
      "\n",
      "step 2\n",
      "{b}->{c}\n",
      "used\n",
      "  (b:wR9p)\n",
      "produced\n",
      "  (c:dh6J)\n",
      "\n",
      "step 3\n",
      "{b}->{x}\n",
      "used\n",
      "  (b:wR9p)\n",
      "produced\n",
      "  (x:lpQn)\n",
      "\n",
      "//\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "import os, sys\n",
    "from typing import Any, Generator, Iterable, Literal\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "\n",
    "class Namespace:\n",
    "    def __init__(self, key_length=4, seed: int=42, key_from_order=False) -> None:\n",
    "        self.node_signatures: dict[int, str] = {}\n",
    "        self._last_k: int = 0\n",
    "        generator = KeyGenerator(seed=seed)\n",
    "        self._kg = generator\n",
    "        self._key_from_order = key_from_order\n",
    "        self._KLEN = key_length\n",
    "        self._MAX_K = len(self._kg.vocab)**self._KLEN\n",
    "        self.transforms: dict[str, Transform] = {}\n",
    "\n",
    "    def NewKey(self):\n",
    "        self._last_k += 1\n",
    "        assert self._last_k < self._MAX_K\n",
    "        if self._key_from_order:\n",
    "            key = self._kg.FromInt(self._last_k, self._KLEN)\n",
    "        else:\n",
    "            key = self._kg.GenerateUID(self._KLEN)\n",
    "        return self._last_k, key\n",
    "\n",
    "    def NewTransform(self, name: str|None = None):\n",
    "        if name is None:\n",
    "            name = len(self.transforms)\n",
    "        t = Transform(self)\n",
    "        self.transforms[name] = t\n",
    "        return t\n",
    "\n",
    "class Hashable:\n",
    "    def __init__(self, ns: Namespace) -> None:\n",
    "        self.namespace = ns\n",
    "        self.hash, self.key = ns.NewKey()\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return self.hash\n",
    "    \n",
    "    def __eq__(self, __value: object) -> bool:\n",
    "        K = \"key\"\n",
    "        return hasattr(__value, K) and self.key == getattr(__value, K)\n",
    "\n",
    "class Node(Hashable):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ns: Namespace,\n",
    "        properties: set[str],\n",
    "        parents: set[Node],\n",
    "    ) -> None:\n",
    "        super().__init__(ns)\n",
    "        self.namespace = ns\n",
    "        self.properties = properties\n",
    "        self.parents = parents\n",
    "        self._sig: str|None = None\n",
    "        # self._diffs = set()\n",
    "        # self._sames = set()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"({'-'.join(self.properties)}:{self.key})\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self}\"\n",
    "    \n",
    "    def IsA(self, other: Node) -> bool:\n",
    "        # if other.key in self._diffs: return False\n",
    "        # if other.key in self._sames: return True\n",
    "        if not other.properties.issubset(self.properties):\n",
    "            # self._diffs.add(other.key)\n",
    "            return False\n",
    "        # self._sames.add(other.key)\n",
    "        # if compare_lineage: return  other.parents.issubset(self.parents)\n",
    "        return True\n",
    "\n",
    "    def Signature(self):\n",
    "        if self._sig is None:\n",
    "            psig = \",\".join(sorted(p.Signature() for p in self.parents))\n",
    "            sig = \",\".join(sorted(self.properties))\n",
    "            self._sig = f'{sig}:[{psig}]' if len(self.parents)>0 else sig\n",
    "        return self._sig\n",
    "\n",
    "class Dependency(Node):\n",
    "    def __init__(self, namespace: Namespace, properties: set[str], parents: set[Node]) -> None:\n",
    "        super().__init__(namespace, properties, parents)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"(D:{'-'.join(self.properties)})\"\n",
    "    \n",
    "class Endpoint(Node):\n",
    "    def __init__(self, namespace: Namespace, properties: set[str], parents: dict[Endpoint, Node]=dict()) -> None:\n",
    "        super().__init__(namespace, properties, set(parents))\n",
    "        self._parent_map = parents # real, proto\n",
    "\n",
    "    def Iterparents(self):\n",
    "        \"\"\"real, prototype\"\"\"\n",
    "        for e, p in self._parent_map.items():\n",
    "            yield e, p\n",
    "\n",
    "class Transform(Hashable):\n",
    "    def __init__(self, ns: Namespace) -> None:\n",
    "        super().__init__(ns)\n",
    "        self.requires: list[Dependency] = list()\n",
    "        self.produces: list[Dependency] = list()\n",
    "        self.deletes: set[Dependency] = set()\n",
    "        self._ns = ns\n",
    "        self._input_group_map: dict[int, list[Dependency]] = {}\n",
    "        self._key = ns.NewKey()\n",
    "        self._seen: set[str] = set()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        def _props(d: Dependency):\n",
    "            return \"{\"+\"-\".join(d.properties)+\"}\"\n",
    "        return f\"{','.join(_props(r) for r in self.requires)}->{','.join(_props(p) for p in self.produces)}\"\n",
    "\n",
    "    def __repr__(self): return f\"{self}\"\n",
    "\n",
    "    def AddRequirement(self, properties: Iterable[str], parents: set[Dependency]=set()):\n",
    "        prototype = Dependency(properties=set(properties), parents=parents, namespace=self._ns)\n",
    "        return self._add_dependency(self.requires, prototype)\n",
    "\n",
    "    def AddProduct(self, properties: Iterable[str], parents: set[Dependency]=set()):\n",
    "        prototype = Dependency(properties=set(properties), parents=parents, namespace=self._ns)\n",
    "        for d in self.deletes:\n",
    "            assert not prototype.IsA(d), f\"can not produce and delete product[{d}]\"\n",
    "        return self._add_dependency(self.produces, prototype)\n",
    "    \n",
    "    # def AddDeletion(self, to_delete: Dependency):\n",
    "    #     print(\"warning!, deletion not implemented\")\n",
    "    #     assert to_delete in self.requires, f\"{to_delete} not in requirements\"\n",
    "    #     if to_delete in self.deletes: return # already added\n",
    "    #     for p in self.produces:\n",
    "    #         assert not p.IsA(to_delete), f\"can not produce and delete product [{p}]\"\n",
    "    #     self.deletes.add(to_delete)\n",
    "\n",
    "    def _add_dependency(self, destination: list[Dependency], prototype: Dependency):\n",
    "        # _dep = Dependency(properties=set(properties), parents=_parents, namespace=self._ns)\n",
    "        _dep = prototype\n",
    "        _parents = _dep.parents\n",
    "        # assert not any(e.IsA(_dep) for e in destination), f\"prev. dep ⊆ new dep\"\n",
    "        # assert not any(_dep.IsA(e) for e in destination), f\"new dep ⊆ prev. dep \"\n",
    "        # destination.add(_dep)\n",
    "        destination.append(_dep)\n",
    "        if destination == self.requires:\n",
    "            i = len(self.requires)-1\n",
    "            for p in _parents:\n",
    "                assert p in self.requires, f\"{p} not added as a requirement\"\n",
    "            self._input_group_map[i] = self._input_group_map.get(i, [])+list(_parents)\n",
    "        return _dep\n",
    "\n",
    "    def _sig(self, endpoints: Iterable[Endpoint]):\n",
    "        # return \"\".join(e.key for e in endpoints)\n",
    "        return self.key+\"-\"+ \"\".join(e.key for e in endpoints)\n",
    "\n",
    "    # just all possibilities regardless of lineage\n",
    "    def Possibilities(self, have: set[Endpoint], constraints: dict[Dependency, Endpoint]=dict()) -> Generator[list[Endpoint], Any, None]:\n",
    "        matches: list[list[Endpoint]] = []\n",
    "        constraints_used = False\n",
    "        for req in self.requires:\n",
    "            if req in constraints:\n",
    "                must_use = constraints[req]\n",
    "                _m = [must_use]\n",
    "            else:\n",
    "                _m = [m for m in have if m.IsA(req)]\n",
    "            if len(_m) == 0: return None\n",
    "            matches.append(_m)\n",
    "        if len(constraints)>0 and not constraints_used: return None\n",
    "\n",
    "        indexes = [0]*len(matches)\n",
    "        indexes[0] = -1\n",
    "        def _advance():\n",
    "            i = 0\n",
    "            while True:\n",
    "                indexes[i] += 1\n",
    "                if indexes[i] < len(matches[i]): return True\n",
    "                indexes[i] = 0\n",
    "                i += 1\n",
    "                if i >= len(matches): return False\n",
    "        while _advance():\n",
    "            yield [matches[i][j] for i, j in enumerate(indexes)]\n",
    "    \n",
    "    # filter possibilities based on correct lineage\n",
    "    def Valids(self, matches: Iterable[list[Endpoint]]):\n",
    "        black_list: set[tuple[int, Endpoint]] = set()\n",
    "        white_list: set[tuple[int, Endpoint]] = set()\n",
    "\n",
    "        choosen: list[Endpoint] = []\n",
    "        for config in matches:\n",
    "            ok = True\n",
    "            for i, (e, r) in enumerate(zip(config, self.requires)):\n",
    "                k = (i, e)\n",
    "                if k in black_list: ok=False; break\n",
    "                if k in white_list: continue\n",
    "                \n",
    "                parents = self._input_group_map.get(i, [])\n",
    "                if len(parents) == 0: # no lineage req.\n",
    "                    white_list.add(k)\n",
    "                    continue\n",
    "                \n",
    "                for prototype in parents:\n",
    "                    # parent must already be in choosen, since it must have been added\n",
    "                    # as a req. before being used as a parent during setup\n",
    "                    found = False\n",
    "                    for p in choosen:\n",
    "                        if not p.IsA(prototype): continue\n",
    "                        if p in e.parents: found=True; break\n",
    "                    if not found: black_list.add(k); ok=False; break\n",
    "                if not ok: break\n",
    "            if ok: yield config\n",
    "\n",
    "    def Apply(self, inputs: Iterable[tuple[Endpoint, Node]]):\n",
    "        # deleted = {}\n",
    "        # for r, (e, e_proto) in zip(self.requires, inputs):\n",
    "        #     assert e.IsA(r), f\"{e_proto}, {e}, {r}\"\n",
    "        #     if r in self.deletes: deleted[e] = e_proto\n",
    "\n",
    "        inputs_dict = dict(inputs)\n",
    "        parent_dict: dict[Any, Any] = {}\n",
    "        for e, _ in inputs_dict.items():\n",
    "            for p, pproto in e.Iterparents():\n",
    "                if p in parent_dict: continue\n",
    "                parent_dict[p] = pproto\n",
    "        for e, eproto in inputs_dict.items():\n",
    "            parent_dict[e] = eproto\n",
    "        produced = {\n",
    "            Endpoint(\n",
    "                namespace=self._ns,\n",
    "                properties=out.properties,\n",
    "                parents=parent_dict\n",
    "            ):out\n",
    "        for out in self.produces}\n",
    "        # return Application(self, inputs_dict, produced, deleted)\n",
    "        return Application(self, inputs_dict, produced)\n",
    "\n",
    "@dataclass\n",
    "class Application:\n",
    "    transform: Transform\n",
    "    used: dict[Endpoint, Node]\n",
    "    produced: dict[Endpoint, Dependency]\n",
    "    # deleted: dict[Endpoint, Node]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        # return f\"{self.transform} || {','.join(str(e) for e in self.used.keys())} -> {','.join(str(e) for e in self.produced)} |x {','.join(str(e) for e in self.deleted)}\"\n",
    "        return f\"{self.transform} || {','.join(str(e) for e in self.used.keys())}->{','.join(str(e) for e in self.produced)}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self}\"\n",
    "\n",
    "@dataclass\n",
    "class HasSteps:\n",
    "    steps: int\n",
    "\n",
    "@dataclass\n",
    "class Result(HasSteps):\n",
    "    application: Application\n",
    "    dependency_plan: list[Application]\n",
    "    \n",
    "@dataclass\n",
    "class DepResult(HasSteps):\n",
    "    plan: list[Application]\n",
    "    endpoint: Endpoint\n",
    "\n",
    "def Solve(given: Iterable[Endpoint], target: Transform, transforms: Iterable[Transform], _debug=False):\n",
    "    @dataclass\n",
    "    class State:\n",
    "        have: dict[Endpoint, Dependency]\n",
    "        needed: set[Dependency]\n",
    "        target: Dependency|Transform\n",
    "        lineage_requirements: dict[Node, Endpoint]\n",
    "        seen_signatures: set[str]\n",
    "        depth: int\n",
    "\n",
    "    def _get_producers_of(target: Dependency):\n",
    "        for tr in transforms:\n",
    "            for p in tr.produces:\n",
    "                if p.IsA(target):\n",
    "                    yield tr\n",
    "                    break\n",
    "\n",
    "    if _debug:\n",
    "        log_path = Path(\"./cache/debug_log.txt\")\n",
    "        log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        log = open(\"./cache/debug_log.txt\", \"w\")\n",
    "        debug_print = lambda *args: log.write(\" \".join(str(a) for a in args)+\"\\n\") if args[0] != \"END\" else log.close()\n",
    "    else:\n",
    "        debug_print = lambda *args: None\n",
    "\n",
    "    _apply_cache: dict[str, Application] = {}\n",
    "    def _apply(target: Transform, inputs: Iterable[tuple[Endpoint, Node]]):\n",
    "        sig  = \"\".join(e.key+d.key for e, d in inputs)\n",
    "        if sig in _apply_cache:\n",
    "            return _apply_cache[sig]\n",
    "        appl = target.Apply(inputs)\n",
    "        _apply_cache[sig] = appl\n",
    "        return appl\n",
    "\n",
    "    def _satisfies_lineage(tproto: Dependency, candidate: Endpoint):\n",
    "        for tp_proto in tproto.parents:\n",
    "            if all(not p.IsA(tp_proto) for p, _ in candidate.Iterparents()):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    HORIZON=64\n",
    "    def _solve_dep(s: State) -> list[DepResult]:\n",
    "        if s.depth >= HORIZON:\n",
    "            if _debug: debug_print(f\" <-  HORIZON\", s.depth)\n",
    "            return []\n",
    "        target: Dependency = s.target\n",
    "        assert isinstance(target, Dependency), f\"{s.target}, not dep\"\n",
    "        if _debug: debug_print(f\" ->\", s.target, s.lineage_requirements)\n",
    "        if _debug: debug_print(f\"   \", s.have.keys())\n",
    "\n",
    "        candidates:list[DepResult] = []\n",
    "        for e, eproto in s.have.items():\n",
    "            if not e.IsA(target): continue\n",
    "            acceptable = True\n",
    "            for rproto, r in s.lineage_requirements.items():\n",
    "                if e == r: continue\n",
    "                if eproto.IsA(rproto): # e is protype, but explicitly breaks lineage\n",
    "                    acceptable=False; break\n",
    "\n",
    "                for p, pproto in e.Iterparents():\n",
    "                    if rproto.IsA(pproto):\n",
    "                        if p != r:\n",
    "                            acceptable=False; break\n",
    "\n",
    "            if not acceptable:\n",
    "                continue\n",
    "            else:\n",
    "                if _debug: debug_print(f\"    ^candidate\", e, eproto, e.parents)\n",
    "                if _debug: debug_print(f\"    ^reqs.    \", s.lineage_requirements)\n",
    "                candidates.append(DepResult(0, [], e))\n",
    "            # elif quality == 2:\n",
    "            #     if DEBUG: debug_print(f\" <-\", s.target, e, \"DIRECT\")\n",
    "            #     return [DepResult(0, [], e)]\n",
    "\n",
    "        def _add_result(res: Result):\n",
    "            ep: Endpoint|None = None\n",
    "            for e in res.application.produced:\n",
    "                if e.IsA(target):\n",
    "                    ep = e; break\n",
    "            assert isinstance(ep, Endpoint)\n",
    "            if not _satisfies_lineage(target, ep): return\n",
    "            candidates.append(DepResult(\n",
    "                res.steps,\n",
    "                res.dependency_plan+[res.application],\n",
    "                ep,\n",
    "            ))\n",
    "\n",
    "        for tr in _get_producers_of(target):\n",
    "            # if target in tr.deletes: continue\n",
    "            results = _solve_tr(State(s.have, s.needed, tr, s.lineage_requirements, s.seen_signatures, s.depth))\n",
    "            for res in results:\n",
    "                _add_result(res)\n",
    "\n",
    "        if _debug: debug_print(f\" <-\", s.target, f\"{len(candidates)} sol.\", candidates[0].endpoint if len(candidates)>0 else None)\n",
    "        return candidates\n",
    "\n",
    "    _transform_cache: dict[str, list[Result]] = {}\n",
    "    def _solve_tr(s: State) -> list[Result]:\n",
    "        assert isinstance(s.target, Transform), f\"{s.target} not tr\"\n",
    "        target: Transform = s.target\n",
    "        if _debug: debug_print(f\">>>{s.depth:02}\", s.target, s.lineage_requirements)\n",
    "        for h in s.have:\n",
    "            if _debug: debug_print(f\"      \", h)\n",
    "\n",
    "        # memoization\n",
    "        sig = \"\".join(e.key for e in s.have)\n",
    "        sig += f\":{s.target.key}\"\n",
    "        sig += \":\"+\"\".join(e.key for e in s.lineage_requirements.values())\n",
    "        if sig in _transform_cache:\n",
    "            if _debug: debug_print(f\"<<<{s.depth:02} CACHED: {len(_transform_cache[sig])} solutions\")\n",
    "            return _transform_cache[sig]\n",
    "        if sig in s.seen_signatures:\n",
    "            if _debug: debug_print(f\"<<<{s.depth:02} FAIL: is loop\")\n",
    "            return []\n",
    "\n",
    "        plans: list[list[DepResult]] = []\n",
    "        for i, req in enumerate(s.target.requires):\n",
    "            req_p = {}\n",
    "            for proto, e in s.lineage_requirements.items():\n",
    "                if req.IsA(proto): continue\n",
    "                req_p[proto] = e\n",
    "\n",
    "            results = _solve_dep(State(s.have, s.needed|{req}, req, req_p, s.seen_signatures|{sig}, s.depth+1))\n",
    "            \n",
    "            if len(results) == 0:\n",
    "                if _debug: debug_print(f\"<<< FAIL\", s.target, req)\n",
    "                return []\n",
    "            else:\n",
    "                plans.append(results)\n",
    "\n",
    "        def _gather_valid_inputs():\n",
    "            valids: list[list[DepResult]] = []\n",
    "            ii = 0\n",
    "            def _gather(req_i: int, req: Dependency, res: DepResult, deps: dict, used: set[Endpoint], inputs: list[DepResult]):\n",
    "                nonlocal ii; ii += 1         \n",
    "                if _debug: debug_print(f\"          \", deps)\n",
    "                if _debug: debug_print(f\"    ___\", req, req.parents)\n",
    "                if _debug: debug_print(f\"        __\", res.endpoint, list(res.endpoint.Iterparents()))\n",
    "                if res.endpoint in used:\n",
    "                    if _debug: debug_print(f\"    ___ FAIL: duplicate input\", res.endpoint)\n",
    "                    return\n",
    "                # used.add(res.endpoint)\n",
    "\n",
    "                if not _satisfies_lineage(req, res.endpoint):\n",
    "                    if _debug: debug_print(f\"    ___ FAIL: unsatisfied lineage\", req)\n",
    "                    return\n",
    "\n",
    "                for rproto in req.parents:\n",
    "                    r = deps[rproto]\n",
    "                    # if all(not p.IsA(rproto) for p, pproto in res.endpoint.Iterparents()):\n",
    "                    #     if DEBUG: debug_print(f\"    ___ FAIL: unsatisfied lineage\", rproto)\n",
    "                    #     _fail=True; break\n",
    "                    res_parents = list(res.endpoint.Iterparents())\n",
    "                    res_parents.reverse()\n",
    "                    for p, pproto in res_parents:\n",
    "                        if not p.IsA(rproto): continue\n",
    "                        if p!=r:\n",
    "                            if _debug: debug_print(f\"    ___ FAIL: lineage mismatch\", p, r)\n",
    "                            return\n",
    "                        else:\n",
    "                            break # in the case of asm -> bin, the closest ancestor takes priority\n",
    "                # deps[req] = res.endpoint\n",
    "\n",
    "                if req_i >= len(target.requires)-1:\n",
    "                    valids.append(inputs+[res])\n",
    "                else:\n",
    "                    req_i += 1\n",
    "                    for i, next_res in enumerate(plans[req_i]):\n",
    "                        _gather(req_i, target.requires[req_i], next_res, deps|{req:res.endpoint}, used|{res.endpoint}, inputs+[res])\n",
    "            req_i = 0\n",
    "            for i, next_res in enumerate(plans[req_i]):\n",
    "                _gather(0, target.requires[req_i], next_res, {}, set(), [])\n",
    "            total = 1\n",
    "            for s in plans:\n",
    "                total *= len(s)\n",
    "            if _debug: debug_print(f\"    ## {ii} visited, {total} combos\")\n",
    "            return valids\n",
    "\n",
    "        if _debug: debug_print(f\"<<<{s.depth:02}\", s.target, s.lineage_requirements)\n",
    "        if _debug: debug_print(f\"     \", [len(x) for x in plans])\n",
    "        solutions: list[Result] = []\n",
    "        # for inputs in _iter_satisfies():\n",
    "        for inputs in _gather_valid_inputs():\n",
    "            my_appl = _apply(s.target, [(res.endpoint, req) for req, res in zip(s.target.requires, inputs)])\n",
    "            consolidated_plan: list[Application] = []\n",
    "            produced_sigs: set[str] = {p.Signature() for p in my_appl.produced}\n",
    "            # if DEBUG: debug_print(f\"   __\", my_appl)\n",
    "            for res in inputs:\n",
    "                for appl in res.plan:\n",
    "                    if all(p.Signature() in produced_sigs for p in appl.produced): continue\n",
    "                    consolidated_plan.append(appl)\n",
    "                    produced_sigs = produced_sigs.union(p.Signature() for p in appl.produced)\n",
    "            solutions.append(Result(\n",
    "                len(consolidated_plan),\n",
    "                my_appl,\n",
    "                consolidated_plan,\n",
    "            ))\n",
    "            # if DEBUG: debug_print(f\"    *\", my_appl)\n",
    "            # if DEBUG: debug_print(f\"     \", [res.endpoint for res in inputs])\n",
    "            # if DEBUG: debug_print(f\"    .\", target.requires)\n",
    "            # for appl in consolidated_plan:\n",
    "            #     if DEBUG: debug_print(f\"    __\", appl)\n",
    "        if _debug: debug_print(f\"     \", f\"{len(solutions)} sol.\", solutions[0].application.produced if len(solutions)>0 else None)\n",
    "        solutions = sorted(solutions, key=lambda s: s.steps)\n",
    "        _transform_cache[sig] = solutions\n",
    "        return solutions\n",
    "\n",
    "    input_tr = Transform(target._ns)\n",
    "    given_dict = {g:input_tr.AddProduct(g.properties) for g in given}\n",
    "    res = _solve_tr(State(given_dict, set(), target, {}, set(), 0))\n",
    "    if _debug: debug_print(\"END\")\n",
    "    return res\n",
    "\n",
    "def _set(s: str):\n",
    "    return set(s.split(\", \"))\n",
    " \n",
    "transforms = []\n",
    "NS = Namespace()\n",
    "\n",
    "t = Transform(NS)\n",
    "d = t.AddRequirement(_set(\"a\"))\n",
    "t.AddProduct(_set(\"b\"))\n",
    "# t.AddDeletion(d)\n",
    "transforms.append(t)\n",
    "\n",
    "t = Transform(NS)\n",
    "d = t.AddRequirement(_set(\"b\"))\n",
    "t.AddProduct(_set(\"c\"))\n",
    "# t.AddDeletion(d)\n",
    "transforms.append(t)\n",
    "\n",
    "t = Transform(NS)\n",
    "d = t.AddRequirement(_set(\"b\"))\n",
    "t.AddProduct(_set(\"x\"))\n",
    "transforms.append(t)\n",
    "\n",
    "haves = [Endpoint(NS, _set(r)) for r in [\n",
    "    # \"precog\",\n",
    "    # # \"db, cog\",\n",
    "    # \"db, kegg\",\n",
    "    \"a\",\n",
    "]]\n",
    "\n",
    "target = Transform(NS)\n",
    "r = target.AddRequirement(_set(\"c\"))\n",
    "r = target.AddRequirement(_set(\"x\"))\n",
    "\n",
    "def _print_sol(plan: Result):\n",
    "    for i, step in enumerate(plan.dependency_plan):\n",
    "        print(f\"step {i+1}\")\n",
    "        print(step.transform)\n",
    "        print(\"used\")\n",
    "        for x in step.used:\n",
    "            print(\" \", x)\n",
    "        print(\"produced\")\n",
    "        for x in step.produced:\n",
    "            print(\" \", x)\n",
    "        # if len(step.deleted) > 0:\n",
    "        #     print(\"deleted\")\n",
    "        #     for x in step.deleted:\n",
    "        #         print(\" \", x)\n",
    "        print()\n",
    "\n",
    "solutions = None\n",
    "def _test():\n",
    "    global solutions\n",
    "    solutions = Solve(haves, target, transforms, _debug=True)\n",
    "    print(f\"{len(solutions)} solutions\")\n",
    "    for res in solutions:\n",
    "        _print_sol(res)\n",
    "        # print(res.steps)\n",
    "        # return f\"{len(solutions)} solutions\", res.dependency_plan+[res.application]\n",
    "    print(\"//\")\n",
    "    # if res is not None:\n",
    "_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, json\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 solutions\n",
      "step 1\n",
      "{Format=FASTA-Data=DNA sequence}->{Data=Protein sequence-Format=GFF3}\n",
      "used\n",
      "  (Format=FASTA-Data=DNA sequence:OVtA)\n",
      "produced\n",
      "  (Data=Protein sequence-Format=GFF3:QAaL)\n",
      "\n",
      "step 2\n",
      "{Data=Protein sequence-Format=GFF3},{Format=Binary format-Data=diamond reference db}->{Data=Protein features-Format=CSV}\n",
      "used\n",
      "  (Data=Protein sequence-Format=GFF3:QAaL)\n",
      "  (Format=Binary format-Data=diamond reference db:bAYL)\n",
      "produced\n",
      "  (Data=Protein features-Format=CSV:PGmm)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# edam ontology\n",
    "# https://bioportal.bioontology.org/ontologies/EDAM?p=classes\n",
    "with open(\"../transforms/simple.1/index.yml\") as yml:\n",
    "    d = yaml.load(yml, Loader=yaml.FullLoader)\n",
    "\n",
    "prototypes = {}\n",
    "r_proto = {}\n",
    "for k, v in d[\"prototypes\"].items():\n",
    "    props = set(v)\n",
    "    prototypes[k] = props\n",
    "    rk = \"\".join(sorted(props))\n",
    "    r_proto[rk] = k\n",
    "\n",
    "seed = abs(hash(json.dumps(d)))\n",
    "NS = Namespace(seed=seed)\n",
    "for tr_name, tr_data in d[\"transforms\"].items():\n",
    "    t = NS.NewTransform(tr_name)\n",
    "    for r in tr_data[\"inputs\"]:\n",
    "        t.AddRequirement(prototypes[r])\n",
    "    for p in tr_data[\"outputs\"]:\n",
    "        t.AddProduct(prototypes[p])\n",
    "\n",
    "target = NS.NewTransform(\"target\")\n",
    "have = []\n",
    "have_lineage = set()\n",
    "for proto in [\n",
    "    \"contigs\",\n",
    "    \"diamond_db\",\n",
    "]:\n",
    "    have.append(Endpoint(NS, prototypes[proto]))\n",
    "    # dep = target.AddRequirement(prototypes[proto])\n",
    "    # have_lineage.add(dep)\n",
    "target.AddRequirement(prototypes[\"orf_annotation\"], have_lineage)\n",
    "\n",
    "solutions = Solve(have, target, list(NS.transforms.values()), _debug=True)\n",
    "\n",
    "print(f\"{len(solutions)} solutions\")\n",
    "_print_sol(solutions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pprodigal\n",
      ">contigs:(D:Format=FASTA-Data=DNA sequence)\n",
      "<orfs:(D:Data=Protein sequence-Format=GFF3)\n",
      "\n",
      "diamond\n",
      ">orfs:(D:Data=Protein sequence-Format=GFF3)\n",
      ">diamond_db:(D:Format=Binary format-Data=diamond reference db)\n",
      "<orf_annotation:(D:Data=Protein features-Format=CSV)\n",
      "\n",
      "target\n",
      ">orf_annotation:(D:Data=Protein features-Format=CSV)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_name(dep: Dependency):\n",
    "    dk = \"\".join(sorted(dep.properties))\n",
    "    return r_proto.get(dk)\n",
    "\n",
    "TAB = \" \"*4\n",
    "for k, tr in NS.transforms.items():\n",
    "    def _dep(deps: list[Dependency], env: str):\n",
    "        names = [get_name(dep) for dep in deps]\n",
    "        var_lines = [TAB+f\"path {n}\" for n in names]\n",
    "        env_lines = []\n",
    "        for i, n in names:\n",
    "            if i == 0:\n",
    "                env_lines.append(f\"{env}={n}\")\n",
    "            else:\n",
    "                env_lines.append(f\"{env}=${env}:{n}\")\n",
    "        return var_lines, env_lines\n",
    "    \n",
    "    print(k)\n",
    "    for dep in tr.requires:\n",
    "        print(f\">{get_name(dep)}:{dep}\")\n",
    "    for dep in tr.produces:\n",
    "        print(f\"<{get_name(dep)}:{dep}\")\n",
    "    print()\n",
    "    # lines = [\n",
    "    #     \"process \"+k+\" {\",\n",
    "    # ]\n",
    "\n",
    "    # in_vars, in_env = _dep(tr.requires, \"input_paths\")\n",
    "    # out_vars, out_env = _dep(tr.produces, \"output\")\n",
    "\n",
    "    # [\n",
    "    #     TAB+f\"path {get_name(dep)}\" for dep in tr.requires\n",
    "    # ] + [\n",
    "    #     \"\",\n",
    "    #     TAB+\"output:\",\n",
    "    # ] + [\n",
    "    #     TAB+f\"path {get_name(dep)}\" for dep in tr.produces\n",
    "    # ] + [\n",
    "    #     \"}\",\n",
    "    #     \"\",\n",
    "    # ]\n",
    "    # print(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
