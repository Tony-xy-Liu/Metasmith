{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import annotations\n",
    "# from pathlib import Path\n",
    "# import yaml\n",
    "# from dataclasses import dataclass, field\n",
    "# from datetime import datetime as dt\n",
    "# import hashlib\n",
    "\n",
    "# from local.constants import WORKSPACE_ROOT\n",
    "\n",
    "# def str_hash(s):\n",
    "#     return int(hashlib.sha256(s.encode(\"utf-8\", \"replace\")).hexdigest(), 16)\n",
    "\n",
    "# @dataclass\n",
    "# class DataType:\n",
    "#     name: str\n",
    "#     properties: dict[str, str]\n",
    "#     library: DataTypeLibrary\n",
    "#     _hash: int = None\n",
    "\n",
    "#     def __hash__(self) -> int:\n",
    "#         if self._hash is None:\n",
    "#             self._hash = str_hash(''.join(self.AsSet()))\n",
    "#         return self._hash\n",
    "    \n",
    "#     @classmethod\n",
    "#     def SetFromDict(cls, raw: dict[str, str]):\n",
    "#         return set(f\"{k}={v}\" for k, v in raw.items())\n",
    "\n",
    "#     def AsSet(self):\n",
    "#         return self.SetFromDict(self.properties)\n",
    "    \n",
    "# @dataclass\n",
    "# class DataTypeLibrary:\n",
    "#     source: Path\n",
    "#     schema: str\n",
    "#     ontology: dict\n",
    "#     types: dict[str, DataType] = field(default_factory=dict)\n",
    "\n",
    "#     def __getitem__(self, key: str) -> DataType:\n",
    "#         return self.types[key]\n",
    "    \n",
    "#     def __in__(self, key: str) -> bool:\n",
    "#         return key in self.types\n",
    "    \n",
    "#     @classmethod\n",
    "#     def Load(cls, path: Path) -> DataTypeLibrary:\n",
    "#         with open(path) as f:\n",
    "#             d = yaml.safe_load(f)\n",
    "#         lib = cls(path, d[\"schema\"], d[\"ontology\"])\n",
    "#         types = {}\n",
    "#         for k, v in d[\"types\"].items():\n",
    "#             types[k] = DataType(\n",
    "#                 name=k,\n",
    "#                 properties=v,\n",
    "#                 library=lib,\n",
    "#             )\n",
    "#         lib.types = types\n",
    "#         return lib\n",
    "\n",
    "# @dataclass\n",
    "# class DataInstance:\n",
    "#     source: Path\n",
    "#     type: DataType\n",
    "#     _hash: int = None\n",
    "\n",
    "#     def __hash__(self) -> int:\n",
    "#         if self._hash is None:\n",
    "#             self._hash = str_hash(str(self.source.resolve())+''.join(self.type.AsSet()))\n",
    "#         return self._hash\n",
    "    \n",
    "#     @classmethod\n",
    "#     def Register(cls, source: Path, type: DataType):\n",
    "#         return cls(source, type)\n",
    "    \n",
    "#     def Pack(self):\n",
    "#         return {\n",
    "#             \"source\": str(self.source),\n",
    "#             \"type\": self.type.name,\n",
    "#             \"properties\": self.type.properties,\n",
    "#         }\n",
    "\n",
    "# @dataclass\n",
    "# class DataInstanceLibrary:\n",
    "#     description: str\n",
    "#     types_library: DataTypeLibrary\n",
    "#     manifest: dict[str, DataInstance] = field(default_factory=dict)\n",
    "#     time_created: dt = field(default_factory=lambda: dt.now())\n",
    "#     time_modified: dt = field(default_factory=lambda: dt.now())\n",
    "\n",
    "#     def __getitem__(self, key: str):\n",
    "#         return self.manifest[key]\n",
    "\n",
    "#     @classmethod\n",
    "#     def Load(cls, path: Path):\n",
    "#         with open(path) as f:\n",
    "#             d = yaml.safe_load(f)\n",
    "\n",
    "#         class_attributes = set(cls.__annotations__.keys())\n",
    "#         TYPE_LIB = \"types_library\"\n",
    "#         d[TYPE_LIB] = DataTypeLibrary.Load(Path(d[TYPE_LIB]))\n",
    "#         for k, v in d.items():\n",
    "#             assert k in class_attributes, f\"unexpected field [{k}]\"\n",
    "#             if k == \"manifest\":\n",
    "#                 manifest = {}\n",
    "#                 for kk, vv in v.items():\n",
    "#                     type = d[TYPE_LIB][vv[\"type\"]]\n",
    "#                     manifest[kk] = DataInstance(\n",
    "#                         source=Path(vv[\"source\"]),\n",
    "#                         type=type,\n",
    "#                     )\n",
    "#                 d[k] = manifest\n",
    "#         return cls(**d)\n",
    "\n",
    "#     def Dump(self, path: Path):\n",
    "#         self.time_modified = dt.now()\n",
    "#         with open(path, \"w\") as f:\n",
    "#             d = {}\n",
    "#             for k, v in self.__dict__.items():\n",
    "#                 if k.startswith(\"_\"): continue\n",
    "#                 if callable(v): continue\n",
    "#                 if k == \"types_library\":\n",
    "#                     v = str(v.source)\n",
    "#                 elif k == \"manifest\":\n",
    "#                     v = {kk: vv.Pack() for kk, vv in v.items()}\n",
    "#                 d[k] = v\n",
    "#             yaml.safe_dump(d, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from metasmith.models.libraries import DataInstance, DataInstanceLibrary, DataTypeLibrary\n",
    "\n",
    "from local.constants import WORKSPACE_ROOT\n",
    "\n",
    "lib = DataTypeLibrary.Load(WORKSPACE_ROOT/\"main/local_mock/prototypes/metagenomics.yml\")\n",
    "given_contigs = DataInstance.Register(WORKSPACE_ROOT/\"scratch/test_ws/data/local/example.fna\", lib[\"contigs\"])\n",
    "given_ref = DataInstance.Register(WORKSPACE_ROOT/\"scratch/test_ws/data/local/uniprot_sprot.dmnd\", lib[\"diamond_protein_reference\"])\n",
    "ilib = DataInstanceLibrary(\n",
    "    description=\"test workspace\",\n",
    "    types_library=lib,\n",
    "    manifest={\n",
    "        \"contigs\": given_contigs,\n",
    "        \"diamond_reference.uniprot_sprot\": given_ref,\n",
    "    }\n",
    ")\n",
    "ilib_path = Path(\"./cache/test.yml\")\n",
    "# ilib.Dump(ilib_path)\n",
    "# ilib2 = DataInstanceLibrary.Load(ilib_path)\n",
    "# ilib2.Dump(ilib_path.with_name(\"test2.yml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metasmith.models.libraries import TransformInstanceLibrary\n",
    "trlib = TransformInstanceLibrary.Load([\n",
    "    Path(\"./transforms/simple_1\"),\n",
    "    # Path(\"./transforms/dupe_test\"),\n",
    "])\n",
    "\n",
    "ilib_path = Path(\"./cache/test.yml\")\n",
    "ilib = DataInstanceLibrary.Load(ilib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is pprodigal!\n",
      "this is diamond!\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from metasmith.models.libraries import DataInstance, DataType, TransformInstanceLibrary, TransformInstance\n",
    "from metasmith.models.solver import *\n",
    "\n",
    "# concretely describes a solver.Application\n",
    "@dataclass\n",
    "class WorkflowStep:\n",
    "    uses: list[DataInstance]\n",
    "    produces: list[DataInstance]\n",
    "    transform: TransformInstance\n",
    "\n",
    "# concretely describes a solver.Result\n",
    "@dataclass\n",
    "class WorkflowPlan:\n",
    "    uses: set[DataInstance]\n",
    "    produces: set[DataInstance]\n",
    "    steps: list[WorkflowStep]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.steps)\n",
    "\n",
    "class WorkflowSolver:\n",
    "    def __init__(\n",
    "            self,\n",
    "            lib: TransformInstanceLibrary,\n",
    "        ) -> None:\n",
    "        self._namespace = Namespace()\n",
    "        self._transform_lib = lib\n",
    "        self._transform_map: dict[Transform, TransformInstance] = {}\n",
    "        self._prototype_instances: dict[Dependency, DataInstance] = {}\n",
    "        def _parse_transform(tr: TransformInstance):\n",
    "            ns = self._namespace\n",
    "            model = Transform(ns)\n",
    "            self._transform_map[model] = tr\n",
    "            for x in tr.input_signature:\n",
    "                model.AddRequirement(x.AsProperties())\n",
    "            for x in tr.output_signature:\n",
    "                dep = model.AddProduct(x.type.AsProperties())\n",
    "                self._prototype_instances[dep] = x\n",
    "            return model\n",
    "        self._transforms = [_parse_transform(t) for p, t in lib]\n",
    "\n",
    "    def Solve(self, given: Iterable[DataInstance], target: Iterable[DataType]):\n",
    "        def _solve(given: Iterable[Endpoint], target: Transform, transforms: Iterable[Transform], _debug=False):\n",
    "            @dataclass\n",
    "            class State:\n",
    "                have: dict[Endpoint, Dependency]\n",
    "                needed: set[Dependency]\n",
    "                target: Dependency|Transform\n",
    "                lineage_requirements: dict[Node, Endpoint]\n",
    "                seen_signatures: set[str]\n",
    "                depth: int\n",
    "\n",
    "            def _get_producers_of(target: Dependency):\n",
    "                for tr in transforms:\n",
    "                    for p in tr.produces:\n",
    "                        if p.IsA(target):\n",
    "                            yield tr\n",
    "                            break\n",
    "\n",
    "            if _debug:\n",
    "                log_path = Path(\"./cache/debug_log.txt\")\n",
    "                log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                log = open(\"./cache/debug_log.txt\", \"w\")\n",
    "                debug_print = lambda *args: log.write(\" \".join(str(a) for a in args)+\"\\n\") if args[0] != \"END\" else log.close()\n",
    "            else:\n",
    "                debug_print = lambda *args: None\n",
    "\n",
    "            _apply_cache: dict[str, Application] = {}\n",
    "            def _apply(target: Transform, inputs: Iterable[tuple[Endpoint, Node]]):\n",
    "                sig  = \"\".join(e.key+d.key for e, d in inputs)\n",
    "                if sig in _apply_cache:\n",
    "                    return _apply_cache[sig]\n",
    "                appl = target.Apply(inputs)\n",
    "                _apply_cache[sig] = appl\n",
    "                return appl\n",
    "\n",
    "            def _satisfies_lineage(tproto: Dependency, candidate: Endpoint):\n",
    "                for tp_proto in tproto.parents:\n",
    "                    if all(not p.IsA(tp_proto) for p, _ in candidate.Iterparents()):\n",
    "                        return False\n",
    "                return True\n",
    "\n",
    "            HORIZON=64\n",
    "            def _solve_dep(s: State) -> list[DependencyResult]:\n",
    "                if s.depth >= HORIZON:\n",
    "                    if _debug: debug_print(f\" <-  HORIZON\", s.depth)\n",
    "                    return []\n",
    "                target: Dependency = s.target\n",
    "                assert isinstance(target, Dependency), f\"{s.target}, not dep\"\n",
    "                if _debug: debug_print(f\" ->\", s.target, s.lineage_requirements)\n",
    "                if _debug: debug_print(f\"   \", s.have.keys())\n",
    "\n",
    "                candidates:list[DependencyResult] = []\n",
    "                for e, eproto in s.have.items():\n",
    "                    if not e.IsA(target): continue\n",
    "                    acceptable = True\n",
    "                    for rproto, r in s.lineage_requirements.items():\n",
    "                        if e == r: continue\n",
    "                        if eproto.IsA(rproto): # e is protype, but explicitly breaks lineage\n",
    "                            acceptable=False; break\n",
    "\n",
    "                        for p, pproto in e.Iterparents():\n",
    "                            if rproto.IsA(pproto):\n",
    "                                if p != r:\n",
    "                                    acceptable=False; break\n",
    "\n",
    "                    if not acceptable:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if _debug: debug_print(f\"    ^candidate\", e, eproto, e.parents)\n",
    "                        if _debug: debug_print(f\"    ^reqs.    \", s.lineage_requirements)\n",
    "                        candidates.append(DependencyResult([], e))\n",
    "                    # elif quality == 2:\n",
    "                    #     if DEBUG: debug_print(f\" <-\", s.target, e, \"DIRECT\")\n",
    "                    #     return [DepResult(0, [], e)]\n",
    "\n",
    "                def _add_result(res: Result):\n",
    "                    ep: Endpoint|None = None\n",
    "                    for e in res.application.produced:\n",
    "                        if e.IsA(target):\n",
    "                            ep = e; break\n",
    "                    assert isinstance(ep, Endpoint)\n",
    "                    if not _satisfies_lineage(target, ep): return\n",
    "                    candidates.append(DependencyResult(\n",
    "                        res.dependency_plan+[res.application],\n",
    "                        ep,\n",
    "                    ))\n",
    "\n",
    "                for tr in _get_producers_of(target):\n",
    "                    # if target in tr.deletes: continue\n",
    "                    results = _solve_tr(State(s.have, s.needed, tr, s.lineage_requirements, s.seen_signatures, s.depth))\n",
    "                    for res in results:\n",
    "                        _add_result(res)\n",
    "\n",
    "                if _debug: debug_print(f\" <-\", s.target, f\"{len(candidates)} sol.\", candidates[0].endpoint if len(candidates)>0 else None)\n",
    "                return candidates\n",
    "\n",
    "            _transform_cache: dict[str, list[Result]] = {}\n",
    "            def _solve_tr(s: State) -> list[Result]:\n",
    "                assert isinstance(s.target, Transform), f\"{s.target} not tr\"\n",
    "                target: Transform = s.target\n",
    "                if _debug: debug_print(f\">>>{s.depth:02}\", s.target, s.lineage_requirements)\n",
    "                for h in s.have:\n",
    "                    if _debug: debug_print(f\"      \", h)\n",
    "\n",
    "                # memoization\n",
    "                sig = \"\".join(e.key for e in s.have)\n",
    "                sig += f\":{s.target.key}\"\n",
    "                sig += \":\"+\"\".join(e.key for e in s.lineage_requirements.values())\n",
    "                if sig in _transform_cache:\n",
    "                    if _debug: debug_print(f\"<<<{s.depth:02} CACHED: {len(_transform_cache[sig])} solutions\")\n",
    "                    return _transform_cache[sig]\n",
    "                if sig in s.seen_signatures:\n",
    "                    if _debug: debug_print(f\"<<<{s.depth:02} FAIL: is loop\")\n",
    "                    return []\n",
    "\n",
    "                plans: list[list[DependencyResult]] = []\n",
    "                for i, req in enumerate(s.target.requires):\n",
    "                    req_p = {}\n",
    "                    for proto, e in s.lineage_requirements.items():\n",
    "                        if req.IsA(proto): continue\n",
    "                        req_p[proto] = e\n",
    "\n",
    "                    results = _solve_dep(State(s.have, s.needed|{req}, req, req_p, s.seen_signatures|{sig}, s.depth+1))\n",
    "                    \n",
    "                    if len(results) == 0:\n",
    "                        if _debug: debug_print(f\"<<< FAIL\", s.target, req)\n",
    "                        return []\n",
    "                    else:\n",
    "                        plans.append(results)\n",
    "\n",
    "                def _gather_valid_inputs():\n",
    "                    valids: list[list[DependencyResult]] = []\n",
    "                    ii = 0\n",
    "                    def _gather(req_i: int, req: Dependency, res: DependencyResult, deps: dict, used: set[Endpoint], inputs: list[DependencyResult]):\n",
    "                        nonlocal ii; ii += 1         \n",
    "                        if _debug: debug_print(f\"          \", deps)\n",
    "                        if _debug: debug_print(f\"    ___\", req, req.parents)\n",
    "                        if _debug: debug_print(f\"        __\", res.endpoint, list(res.endpoint.Iterparents()))\n",
    "                        if res.endpoint in used:\n",
    "                            if _debug: debug_print(f\"    ___ FAIL: duplicate input\", res.endpoint)\n",
    "                            return\n",
    "                        # used.add(res.endpoint)\n",
    "\n",
    "                        if not _satisfies_lineage(req, res.endpoint):\n",
    "                            if _debug: debug_print(f\"    ___ FAIL: unsatisfied lineage\", req)\n",
    "                            return\n",
    "\n",
    "                        for rproto in req.parents:\n",
    "                            r = deps[rproto]\n",
    "                            # if all(not p.IsA(rproto) for p, pproto in res.endpoint.Iterparents()):\n",
    "                            #     if DEBUG: debug_print(f\"    ___ FAIL: unsatisfied lineage\", rproto)\n",
    "                            #     _fail=True; break\n",
    "                            res_parents = list(res.endpoint.Iterparents())\n",
    "                            res_parents.reverse()\n",
    "                            for p, pproto in res_parents:\n",
    "                                if not p.IsA(rproto): continue\n",
    "                                if p!=r:\n",
    "                                    if _debug: debug_print(f\"    ___ FAIL: lineage mismatch\", p, r)\n",
    "                                    return\n",
    "                                else:\n",
    "                                    break # in the case of asm -> bin, the closest ancestor takes priority\n",
    "                        # deps[req] = res.endpoint\n",
    "\n",
    "                        if req_i >= len(target.requires)-1:\n",
    "                            valids.append(inputs+[res])\n",
    "                        else:\n",
    "                            req_i += 1\n",
    "                            for i, next_res in enumerate(plans[req_i]):\n",
    "                                _gather(req_i, target.requires[req_i], next_res, deps|{req:res.endpoint}, used|{res.endpoint}, inputs+[res])\n",
    "                    req_i = 0\n",
    "                    for i, next_res in enumerate(plans[req_i]):\n",
    "                        _gather(0, target.requires[req_i], next_res, {}, set(), [])\n",
    "                    total = 1\n",
    "                    for s in plans:\n",
    "                        total *= len(s)\n",
    "                    if _debug: debug_print(f\"    ## {ii} visited, {total} combos\")\n",
    "                    return valids\n",
    "\n",
    "                if _debug: debug_print(f\"<<<{s.depth:02}\", s.target, s.lineage_requirements)\n",
    "                if _debug: debug_print(f\"     \", [len(x) for x in plans])\n",
    "                solutions: list[Result] = []\n",
    "                # for inputs in _iter_satisfies():\n",
    "                for inputs in _gather_valid_inputs():\n",
    "                    my_appl = _apply(s.target, [(res.endpoint, req) for req, res in zip(s.target.requires, inputs)])\n",
    "                    consolidated_plan: list[Application] = []\n",
    "                    produced_sigs: set[str] = {p.Signature() for p in my_appl.produced}\n",
    "                    # if DEBUG: debug_print(f\"   __\", my_appl)\n",
    "                    for res in inputs:\n",
    "                        for appl in res.plan:\n",
    "                            if all(p.Signature() in produced_sigs for p in appl.produced): continue\n",
    "                            consolidated_plan.append(appl)\n",
    "                            produced_sigs = produced_sigs.union(p.Signature() for p in appl.produced)\n",
    "                    solutions.append(Result(\n",
    "                        my_appl,\n",
    "                        consolidated_plan,\n",
    "                    ))\n",
    "                    # if DEBUG: debug_print(f\"    *\", my_appl)\n",
    "                    # if DEBUG: debug_print(f\"     \", [res.endpoint for res in inputs])\n",
    "                    # if DEBUG: debug_print(f\"    .\", target.requires)\n",
    "                    # for appl in consolidated_plan:\n",
    "                    #     if DEBUG: debug_print(f\"    __\", appl)\n",
    "                if _debug: debug_print(f\"     \", f\"{len(solutions)} sol.\", solutions[0].application.produced if len(solutions)>0 else None)\n",
    "                solutions = sorted(solutions, key=lambda s: len(s))\n",
    "                _transform_cache[sig] = solutions\n",
    "                return solutions\n",
    "\n",
    "            input_tr = Transform(target._ns)\n",
    "            given_dict = {g:input_tr.AddProduct(g.properties) for g in given}\n",
    "            res = _solve_tr(State(given_dict, set(), target, {}, set(), 0))\n",
    "            if _debug: debug_print(\"END\")\n",
    "            return res\n",
    "\n",
    "        data_instances = {Endpoint(self._namespace, x.type.AsProperties()):x for x in given}\n",
    "        given_instances = {k for k in data_instances}\n",
    "        output_map: dict[Dependency, DataType] = {}\n",
    "        target_tr = Transform(self._namespace)\n",
    "        for x in target:\n",
    "            dep = target_tr.AddRequirement(x.AsProperties())\n",
    "            output_map[dep] = x\n",
    "        solutions = _solve(data_instances, target_tr, self._transforms)\n",
    "        if len(solutions) == 0: return\n",
    "\n",
    "        solution = solutions[0] # just pick first solution\n",
    "        steps = []\n",
    "        for appl in solution.dependency_plan:\n",
    "            tr = self._transform_map[appl.transform]\n",
    "            used = [data_instances[x] for x in appl.used]\n",
    "            produced = []\n",
    "            for e, dep in appl.produced.items():\n",
    "                inst = self._prototype_instances[dep]\n",
    "                data_instances[e] = inst\n",
    "                produced.append(inst)\n",
    "            steps.append(WorkflowStep(used, produced, tr))\n",
    "\n",
    "        target_instances = set()\n",
    "        for x in solution.application.used: # targets\n",
    "            target_instances.add(data_instances[x])\n",
    "        return WorkflowPlan(\n",
    "            given_instances,\n",
    "            target_instances,\n",
    "            steps\n",
    "        )\n",
    "\n",
    "solver = WorkflowSolver(trlib)\n",
    "plan = solver.Solve(\n",
    "    [\n",
    "        ilib[\"contigs\"],\n",
    "        ilib[\"diamond_reference.uniprot_sprot\"],\n",
    "    ],\n",
    "    [\n",
    "        lib.types[\"orf_annotations\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "for step in plan.steps:\n",
    "    # for d in step.uses:\n",
    "    #     print(d.type)\n",
    "    step.transform.protocol(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
