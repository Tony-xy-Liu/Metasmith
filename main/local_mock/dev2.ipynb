{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert plan to nextflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from metasmith.solver import WorkflowSolver\n",
    "from metasmith.models.libraries import DataInstanceLibrary, DataTypeLibrary, DataInstance, TransformInstanceLibrary\n",
    "\n",
    "from local.constants import WORKSPACE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = DataTypeLibrary.Load(WORKSPACE_ROOT/\"main/local_mock/prototypes/metagenomics.yml\")\n",
    "trlib = TransformInstanceLibrary.Load([\n",
    "    Path(\"./transforms/simple_1\"),\n",
    "    # Path(\"./transforms/dupe_test\"),\n",
    "])\n",
    "ilib_path = Path(\"./cache/test.yml\")\n",
    "ilib = DataInstanceLibrary.Load(ilib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver = WorkflowSolver(trlib)\n",
    "plan = solver.Solve(\n",
    "    [\n",
    "        ilib[\"contigs\"],\n",
    "        ilib[\"diamond_reference.uniprot_sprot\"],\n",
    "        ilib[\"container.diamond\"],\n",
    "        ilib[\"container.pprodigal\"],\n",
    "    ],\n",
    "    [\n",
    "        lib[\"orf_annotations\"].WithAncestors([ilib[\"contigs\"].type]),\n",
    "    ],\n",
    "    seed=42,\n",
    ")\n",
    "plan is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PZ4TH ['example.fna', 'pprodigal.oci.uri']\n",
      "JBCpR ['diamond.oci.uri', 'uniprot_sprot.dmnd', 'orfs.faa']\n"
     ]
    }
   ],
   "source": [
    "for step in plan.steps:\n",
    "    print(step.key, [x.source.name for x, e in step.uses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WS = Path(\"./cache/ws1/run_1\")\n",
    "BOOTSTRAP_BASH = WS/\"bootstrap.sh\"\n",
    "# os.system(f\"rm -r {WS}\")\n",
    "WS.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cache/ws1/run_1/bootstrap.sh\n"
     ]
    }
   ],
   "source": [
    "from metasmith.agents.bootstrap import Container\n",
    "\n",
    "CONTAINER = Container(\n",
    "    image = \"quay.io/hallamlab/metasmith:0.2.dev-e185f76\",\n",
    "    binds = [\n",
    "        (WORKSPACE_ROOT/\"main/relay_agent/dist\", \"/app\"),\n",
    "        (WORKSPACE_ROOT/\"src/metasmith\", \"/opt/conda/envs/metasmith_env/lib/python3.12/site-packages/metasmith\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "_deployment = Path(\"./.msm\")\n",
    "_relay_path = _deployment/\"relay\"\n",
    "cmd_deploy = CONTAINER.RunCommand(\"metasmith api deploy_from_container\")\n",
    "cmd_start_relay = f\"nohup {_relay_path}/server --io {_relay_path}/connections start >{_deployment}/logs/relay.log 2>&1 &\"\n",
    "cmd_start_task = CONTAINER.RunCommand('metasmith api execute_transform --body \"{\\\\\"context\\\\\": \\\\\"$1\\\\\"}\"')\n",
    "cmd_stop_relay = f\"{_relay_path}/server --io {_relay_path}/connections stop\"\n",
    "with open(BOOTSTRAP_BASH, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash\\n\")\n",
    "    f.write(cmd_deploy + \"\\n\")\n",
    "    f.write(cmd_start_relay + \"\\n\")\n",
    "    f.write(cmd_start_task + \"\\n\")\n",
    "    f.write(cmd_stop_relay + \"\\n\")\n",
    "\n",
    "print(f\"./{BOOTSTRAP_BASH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cache/ws1/run_1/workflow.nf\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from metasmith.models.libraries import ExecutionContext\n",
    "\n",
    "TAB = \" \"*4\n",
    "wf_path = WS/\"workflow.nf\"\n",
    "context_dir = WS/\"contexts\"\n",
    "context_dir.mkdir(parents=True, exist_ok=True)\n",
    "contexts = []\n",
    "process_definitions = {}\n",
    "workflow_definition = []\n",
    "target_endpoints = {e for x, e in plan.targets}\n",
    "for step in plan.steps:\n",
    "    name = f\"{step.transform.source.stem}__{step.transform_key}\"\n",
    "    if name not in process_definitions:\n",
    "        src = [f\"process {name}\"+\" {\"]\n",
    "        to_pubish = [x for x, e in step.produces if e in target_endpoints]\n",
    "        for x in to_pubish:\n",
    "            src.append(TAB+f'publishDir \"$params.output\", mode: \"copy\", pattern: \"{x.source}\"')\n",
    "        if len(to_pubish)>0:\n",
    "            src.append(\"\") # newline\n",
    "\n",
    "        src += [\n",
    "            TAB+\"input:\",\n",
    "            TAB+TAB+f'path bootstrap',\n",
    "            TAB+TAB+f'path context',\n",
    "        ] + [\n",
    "            TAB+TAB+f'path _{x.type.name}' for x, e in step.uses\n",
    "        ] + [\n",
    "            \"\",\n",
    "            TAB+\"output:\",\n",
    "        ] + [\n",
    "            TAB+TAB+f'path \"{x.source}\"' for x, e in step.produces\n",
    "        ] + [\n",
    "            \"\",\n",
    "            TAB+'script:',\n",
    "            TAB+'\"\"\"',\n",
    "        ] + [\n",
    "            TAB+f'echo \"{x.type.name},'+'${_'+x.type.name+'}\" >>inputs.manifest' for x, e in step.uses\n",
    "        ] + [\n",
    "            TAB+f'bash $bootstrap $context',\n",
    "            TAB+'\"\"\"',\n",
    "            \"}\"\n",
    "        ]\n",
    "        process_definitions[name] = \"\\n\".join(src)\n",
    "\n",
    "    type_libraries = [x.type.library.source for x, e in step.uses]\n",
    "    type_libraries = {p.stem: p for p in type_libraries}\n",
    "    type_libraries = list(type_libraries.values())\n",
    "    context = ExecutionContext(\n",
    "        inputs = {e.key:x for x, e in step.uses},\n",
    "        outputs = {e.key:x for x, e in step.produces},\n",
    "        transform_definition = step.transform.source,\n",
    "        type_libraries = type_libraries,\n",
    "    )\n",
    "    context_path = context_dir/f\"{step.key}.yml\"\n",
    "    with open(context_path, \"w\") as f:\n",
    "        yaml.dump(context.Pack(), f)\n",
    "    contexts.append((step.key, context_path))\n",
    "\n",
    "    output_vars = [f\"_{e.key}\" for x, e in step.produces]\n",
    "    output_vars = ', '.join(output_vars)\n",
    "    if len(step.produces) > 1:\n",
    "        output_vars = f\"({output_vars})\"\n",
    "    input_vars = ['bootstrap', f'context_{step.key}']+[f\"_{e.key}\" for x, e in step.uses]\n",
    "    input_vars = ', '.join(input_vars)\n",
    "    workflow_definition.append(TAB+f'{output_vars} = {name}({input_vars})')\n",
    "\n",
    "workflow_definition = [\n",
    "    \"workflow {\",\n",
    "    TAB+f'bootstrap = Channel.fromPath(\"{BOOTSTRAP_BASH.resolve()}\")',\n",
    "] + [\n",
    "    TAB+f'context_{k} = Channel.fromPath(\"{p.resolve()}\")' for k, p in contexts\n",
    "] + [\n",
    "    \"\",\n",
    "] + [\n",
    "    TAB+f'_{e.key}'+f' = Channel.fromPath(\"{x.source.resolve()}\") // {x.type.name}' for x, e in plan.given\n",
    "] + [\n",
    "    \"\",\n",
    "] + workflow_definition + [\n",
    "    \"}\",\n",
    "]\n",
    "\n",
    "wf_contents = [\n",
    "    \"\\n\\n\".join(process_definitions.values()),\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\".join(workflow_definition),\n",
    "    \"\\n\",\n",
    "]\n",
    "wf_contents = ''.join(wf_contents)\n",
    "with open(wf_path, \"w\") as f:\n",
    "    f.write(wf_contents)\n",
    "\n",
    "print(f\"./{wf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mNextflow 24.10.4 is available - Please consider updating your version to it\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " N E X T F L O W   ~  version 24.10.2\n",
      "\n",
      "WARN: It appears you have never run this project before -- Option `-resume` is ignored\n",
      "Launching `/home/tony/workspace/tools/Metasmith/main/local_mock/cache/ws1/run_1/workflow.nf` [irreverent_visvesvaraya] DSL2 - revision: 031f5e3c50\n",
      "\n",
      "Plus 2 more processes waiting for tasks…\n",
      "\n",
      "executor >  slurm (1)\n",
      "[6f/8c0fce] pprod | 0 of 1\n",
      "Plus 1 more processes waiting for tasks…\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(f\"\"\"\\\n",
    "PATH={WORKSPACE_ROOT/\"main/local_mock/mock\"}:$PATH\n",
    "cd {WS.resolve()}\n",
    "nextflow -C ../../../config/nxf_slurm.nf \\\n",
    "    -log {(WS/\"logs\").resolve()}/log \\\n",
    "    run {wf_path.resolve()} \\\n",
    "    -resume \\\n",
    "    -work-dir {(WS/\"work\").resolve()} \\\n",
    "    --account dummy_slurm_account\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
